{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyvi import ViTokenizer\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = 'Train_Full'\n",
    "tree_path = os.walk(ROOT_PATH)\n",
    "file_list = []\n",
    "\n",
    "for (dirpath, dirname, filenames) in tree_path:\n",
    "    if dirpath == ROOT_PATH: \n",
    "        continue\n",
    "    for filename in filenames:\n",
    "        file_list.append(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Train_Full\\\\Chinh tri Xa hoi\\\\XH_NLD_T_ (8997).txt',\n",
       " 'Train_Full\\\\Chinh tri Xa hoi\\\\XH_NLD_T_ (8999).txt',\n",
       " 'Train_Full\\\\Chinh tri Xa hoi\\\\XH_NLD_T_ (9001).txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tôi', 'là', 'sinh_viên']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hàm để loại bỏ các ký tự biểu cảm\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "# Chuyển thành chữ viết thường, bỏ biểu bảm và các ký tự đặc biệt\n",
    "# sau đó sử dụng Vitokenizer là bộ tokennize cho tiếng Việt có hỗ trợ word segmentation\n",
    "def make_clean(string, to_tokens=True):\n",
    "    res = string.lower()\n",
    "    res = remove_emoji(res)\n",
    "    res = re.sub(r'[!“”\"#$%&\\()*+,-./:;<=>?@[\\]^_`{|}~]', \" \", res)\n",
    "    res = re.sub(r'\\s+', ' ', res).strip()\n",
    "    res = ViTokenizer.tokenize(res)\n",
    "    if to_tokens:\n",
    "        res = res.split()\n",
    "    return res\n",
    "\n",
    "make_clean('Tôi     là(*(*%$sinh (*&viên', to_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 132.66it/s]\n"
     ]
    }
   ],
   "source": [
    "LOOKBACK_LENGTH = 50 # sử dụng 50 từ trước để dự đoán 1 từ tiếp theo\n",
    "sequences = []\n",
    "for filepath in tqdm(file_list[:1000]): # phần demo này mình chỉ sử dụng 1,000 file đầu tiên mà mình đọc được để chạy nhẹ máy (do sức mạnh máy không đáp ứng hết cả bộ dữ liệu - mọi người có thể sử dụng colab và bật GPU để chạy nhanh hơn nhé!)\n",
    "    with open(filepath, encoding='utf-16') as f:\n",
    "        p = f.read()\n",
    "        tokens = make_clean(p, to_tokens=True)\n",
    "        for i in range(len(tokens) - LOOKBACK_LENGTH):\n",
    "            seq = tokens[i:i+LOOKBACK_LENGTH+1]\n",
    "            sequences.append(' '.join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lưu dữ liệu vào file train.txt\n",
    "with open('train.txt', 'w', encoding='utf-16') as f:\n",
    "    for seq in sequences:\n",
    "        f.write(seq+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
